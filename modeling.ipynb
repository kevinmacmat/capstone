{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmacmat/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1ed8b5bc84eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from project_functions import *\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "spacy.load('en')\n",
    "parser = English()\n",
    "\n",
    "# Spacy transformer\n",
    "import thinc\n",
    "import random\n",
    "import GPUtil\n",
    "import torch\n",
    "from spacy.util import minibatch\n",
    "from tqdm.auto import tqdm\n",
    "import unicodedata\n",
    "import wasabi\n",
    "import numpy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/sqr_comments_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text function from project_functions file. Removes punctuation, whitespace, numbers, and makes text lowercase\n",
    "cleanupText(df, 'comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample minority class to address class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate minority and majority classes\n",
    "negative = df[df.compound_binary==0]\n",
    "positive = df[df.compound_binary==1]\n",
    "\n",
    "# upsample minority\n",
    "negative_upsampled = resample(negative,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(positive), # match number in majority class\n",
    "                          random_state=23) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "df = pd.concat([positive, negative_upsampled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN/TEST SPLIT\n",
    "X = df['comments'].values\n",
    "y = df['compound_binary'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate, fit, and encode using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_data_train = vectorizer.fit_transform(X_train)\n",
    "tf_idf_data_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmacmat/opt/anaconda3/envs/capstone_env/lib/python3.7/site-packages/sklearn/dummy.py:132: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
      "  \"stratified to prior in 0.24.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Fitting & predicting the Dummy Classifier (Baseline Model)\n",
    "dclf = DummyClassifier() \n",
    "dclf.fit(tf_idf_data_train, y_train)\n",
    "dummy_test_preds = dclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Test F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.505051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  Precision    Recall   Test F1\n",
       "0  Dummy Classifier   0.490196  0.520833  0.505051"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get scores\n",
    "dummy_prec_test_score = precision_score(y_test, dummy_test_preds)\n",
    "dummy_recall_test_score = recall_score(y_test, dummy_test_preds)\n",
    "dummy_f1_test_score = f1_score(y_test, dummy_test_preds, average='macro')\n",
    "dummy_scores = pd.DataFrame({'Model':['Dummy Classifier'], 'Precision':[dummy_prec_test_score], 'Recall':[dummy_recall_test_score], 'Test F1':[dummy_f1_test_score]})\n",
    "dummy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Train Recall</th>\n",
       "      <th>Test Recall</th>\n",
       "      <th>Train F1</th>\n",
       "      <th>Test F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naieve Bayes</td>\n",
       "      <td>0.998311</td>\n",
       "      <td>0.943878</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.963542</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.954527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  Train Precision  Test Precision  Train Recall  Test Recall  \\\n",
       "0  Naieve Bayes         0.998311        0.943878         0.985     0.963542   \n",
       "\n",
       "   Train F1   Test F1  \n",
       "0  0.991582  0.954527  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate Naieve Bayes Classifier\n",
    "nb_classifier = MultinomialNB(alpha=.03)\n",
    "\n",
    "# Predict using Naieve Bayes Classifier\n",
    "nb_classifier.fit(tf_idf_data_train, y_train)\n",
    "nb_train_preds = nb_classifier.predict(tf_idf_data_train)\n",
    "nb_test_preds = nb_classifier.predict(tf_idf_data_test)\n",
    "\n",
    "# Get scores\n",
    "nb_prec_train_score = precision_score(y_train, nb_train_preds)\n",
    "nb_prec_test_score = precision_score(y_test, nb_test_preds)\n",
    "nb_recall_train_score = recall_score(y_train, nb_train_preds)\n",
    "nb_recall_test_score = recall_score(y_test, nb_test_preds)\n",
    "nb_f1_train_score = f1_score(y_train, nb_train_preds, average='macro')\n",
    "nb_f1_test_score = f1_score(y_test, nb_test_preds, average='macro')\n",
    "nb_scores = pd.DataFrame({'Model':['Naieve Bayes'], 'Train Precision':[nb_prec_train_score], 'Test Precision':[nb_prec_test_score], 'Train Recall':[nb_recall_train_score], 'Test Recall':[nb_recall_test_score], 'Train F1':[nb_f1_train_score], 'Test F1':[nb_f1_test_score]})\n",
    "nb_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Train Recall</th>\n",
       "      <th>Test Recall</th>\n",
       "      <th>Train F1</th>\n",
       "      <th>Test F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.996593</td>\n",
       "      <td>0.944751</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.98569</td>\n",
       "      <td>0.921452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Train Precision  Test Precision  Train Recall  Test Recall  \\\n",
       "0  Random Forest         0.996593        0.944751         0.975     0.890625   \n",
       "\n",
       "   Train F1   Test F1  \n",
       "0   0.98569  0.921452  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=1000, min_samples_leaf=.001, n_jobs=-1)\n",
    "# rf_classifier = RandomForestClassifier(max_depth=20, n_estimators=1000, min_samples_leaf=.001, n_jobs=-1)\n",
    "\n",
    "# Predict using Random Forest Classifier\n",
    "rf_classifier.fit(tf_idf_data_train, y_train)\n",
    "rf_train_preds = rf_classifier.predict(tf_idf_data_train)\n",
    "rf_test_preds = rf_classifier.predict(tf_idf_data_test)\n",
    "\n",
    "# Get scores\n",
    "rf_prec_train_score = precision_score(y_train, rf_train_preds)\n",
    "rf_prec_test_score = precision_score(y_test, rf_test_preds)\n",
    "rf_recall_train_score = recall_score(y_train, rf_train_preds)\n",
    "rf_recall_test_score = recall_score(y_test, rf_test_preds)\n",
    "rf_f1_train_score = f1_score(y_train, rf_train_preds, average='macro')\n",
    "rf_f1_test_score = f1_score(y_test, rf_test_preds, average='macro')\n",
    "rf_scores = pd.DataFrame({'Model':['Random Forest'], 'Train Precision':[rf_prec_train_score], 'Test Precision':[rf_prec_test_score], 'Train Recall':[rf_recall_train_score], 'Test Recall':[rf_recall_test_score], 'Train F1':[rf_f1_train_score], 'Test F1':[rf_f1_test_score]})\n",
    "rf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Classifier using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train, test = train_test_split(df, random_state=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy's standard transformer\n",
    "STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]\n",
    "\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "def cleanText(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def tokenizeText(sample):\n",
    "    tokens = parser(sample)\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Train Recall</th>\n",
       "      <th>Test Recall</th>\n",
       "      <th>Train F1</th>\n",
       "      <th>Test F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVC w/ Spacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994624</td>\n",
       "      <td>0.99665</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.972182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Train Precision  Test Precision  Train Recall  \\\n",
       "0  Linear SVC w/ Spacy              1.0        0.994624       0.99665   \n",
       "\n",
       "   Test Recall  Train F1   Test F1  \n",
       "0     0.948718  0.998316  0.972182  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate vectorizer, classifier, and pipeline\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizeText)\n",
    "clf = LinearSVC(tol=1e-5, C=1, dual=True, max_iter=2000)\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
    "\n",
    "# Create training and testing dependent/independent variables\n",
    "train1 = train['comments'].tolist()\n",
    "print(train1)\n",
    "labelsTrain1 = train['compound_binary'].tolist()\n",
    "\n",
    "test1 = test['comments'].tolist()\n",
    "labelsTest1 = test['compound_binary'].tolist()\n",
    "\n",
    "\n",
    "# Fit the LinearSVC pipeline to the training data\n",
    "pipe.fit(train1, labelsTrain1)\n",
    "\n",
    "# Training predictions \n",
    "train_preds = pipe.predict(train1)\n",
    "svc_prec_train_score = precision_score(labelsTrain1, train_preds)\n",
    "svc_recall_train_score = recall_score(labelsTrain1, train_preds)\n",
    "svc_f1_train_score = f1_score(labelsTrain1, train_preds, average='macro')\n",
    "\n",
    "# Testing predictions\n",
    "preds = pipe.predict(test1)\n",
    "svc_prec_test_score = precision_score(labelsTest1, preds)\n",
    "svc_recall_test_score = recall_score(labelsTest1, preds)\n",
    "svc_f1_test_score = f1_score(labelsTest1, preds, average='macro')\n",
    "\n",
    "pd.DataFrame({'Model':['Linear SVC w/ Spacy'], 'Train Precision':[svc_prec_train_score], 'Test Precision':[svc_prec_test_score], 'Train Recall':[svc_recall_train_score], 'Test Recall':[svc_recall_test_score], 'Train F1':[svc_f1_train_score], 'Test F1':[svc_f1_test_score]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Classifier using spaCy Transformer and BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinc\n",
    "import random\n",
    "import GPUtil\n",
    "import torch\n",
    "from spacy.util import minibatch\n",
    "from tqdm.auto import tqdm\n",
    "import unicodedata\n",
    "import wasabi\n",
    "import numpy\n",
    "from collections import Counter\n",
    "from project_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks if GPU is in usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not in use.\n"
     ]
    }
   ],
   "source": [
    "spacy.util.fix_random_seed(0)\n",
    "is_using_gpu = spacy.prefer_gpu()\n",
    "if is_using_gpu:\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "    print(\"GPU Usage\")\n",
    "    GPUtil.showUtilization()\n",
    "else:\n",
    "    print('GPU not in use.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the dataframe because when creating the training and testing text and labels below it will not accept the upsampled data due to duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dbn</th>\n",
       "      <th>school_name</th>\n",
       "      <th>school_type</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>rigor_instruction_rating</th>\n",
       "      <th>collab_teachers_rating</th>\n",
       "      <th>support_environ_rating</th>\n",
       "      <th>effective_lead_rating</th>\n",
       "      <th>fam_comm_ties_rating</th>\n",
       "      <th>trust_rating</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_chronic_absent</th>\n",
       "      <th>teacher_attendance_rate</th>\n",
       "      <th>sqr_rating</th>\n",
       "      <th>borough</th>\n",
       "      <th>comments</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>compound</th>\n",
       "      <th>compound_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01M015</td>\n",
       "      <td>P.S. 015 Roberto Clemente</td>\n",
       "      <td>Elementary</td>\n",
       "      <td>161</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.974</td>\n",
       "      <td>6.0</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>ps  is an extraordinary small school that goes...</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01M019</td>\n",
       "      <td>P.S. 019 Asher Levy</td>\n",
       "      <td>Elementary</td>\n",
       "      <td>239</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.966</td>\n",
       "      <td>6.0</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>wrong person the negative comments dont do jus...</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01M020</td>\n",
       "      <td>P.S. 020 Anna Silver</td>\n",
       "      <td>Elementary</td>\n",
       "      <td>439</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.966</td>\n",
       "      <td>5.0</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>while we have always lived with the notion tha...</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01M034</td>\n",
       "      <td>P.S. 034 Franklin D. Roosevelt</td>\n",
       "      <td>K-8</td>\n",
       "      <td>288</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.968</td>\n",
       "      <td>4.0</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>the doe has tabled a proposal to combine ps  w...</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01M063</td>\n",
       "      <td>The STAR Academy - P.S.63</td>\n",
       "      <td>Elementary</td>\n",
       "      <td>207</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.967</td>\n",
       "      <td>6.0</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>this school is an amazing school because we ha...</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dbn                     school_name school_type  enrollment  \\\n",
       "0  01M015       P.S. 015 Roberto Clemente  Elementary         161   \n",
       "1  01M019             P.S. 019 Asher Levy  Elementary         239   \n",
       "2  01M020            P.S. 020 Anna Silver  Elementary         439   \n",
       "3  01M034  P.S. 034 Franklin D. Roosevelt         K-8         288   \n",
       "4  01M063       The STAR Academy - P.S.63  Elementary         207   \n",
       "\n",
       "   rigor_instruction_rating  collab_teachers_rating  support_environ_rating  \\\n",
       "0                      1.00                     1.0                    1.00   \n",
       "1                      1.00                     1.0                    0.75   \n",
       "2                      0.25                     0.5                    0.50   \n",
       "3                      0.50                     0.5                    0.50   \n",
       "4                      1.00                     1.0                    0.75   \n",
       "\n",
       "   effective_lead_rating  fam_comm_ties_rating  trust_rating  ...  \\\n",
       "0                   1.00                  1.00          0.75  ...   \n",
       "1                   1.00                  1.00          0.75  ...   \n",
       "2                   0.50                  0.75          0.75  ...   \n",
       "3                   0.50                  0.75          0.50  ...   \n",
       "4                   0.75                  1.00          0.75  ...   \n",
       "\n",
       "   pct_chronic_absent  teacher_attendance_rate  sqr_rating    borough  \\\n",
       "0               0.227                    0.974         6.0  manhattan   \n",
       "1               0.343                    0.966         6.0  manhattan   \n",
       "2               0.296                    0.966         5.0  manhattan   \n",
       "3               0.455                    0.968         4.0  manhattan   \n",
       "4               0.347                    0.967         6.0  manhattan   \n",
       "\n",
       "                                            comments    pos    neg    neu  \\\n",
       "0  ps  is an extraordinary small school that goes...  0.173  0.006  0.821   \n",
       "1  wrong person the negative comments dont do jus...  0.154  0.054  0.792   \n",
       "2  while we have always lived with the notion tha...  0.259  0.017  0.724   \n",
       "3  the doe has tabled a proposal to combine ps  w...  0.111  0.107  0.782   \n",
       "4  this school is an amazing school because we ha...  0.181  0.050  0.769   \n",
       "\n",
       "   compound  compound_binary  \n",
       "0    0.9995              1.0  \n",
       "1    0.9959              1.0  \n",
       "2    0.9999              1.0  \n",
       "3    0.7877              1.0  \n",
       "4    0.9992              1.0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/kevinmacmat/Desktop/capstone/csv/sqr_comments_sentiment.csv')\n",
    "cleanupText(df, 'comments')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of tuples column w/ (comments, compound_binary) for each school in order to fit formatting requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (ps  is an extraordinary small school that goe...\n",
       "1    (wrong person the negative comments dont do ju...\n",
       "2    (while we have always lived with the notion th...\n",
       "3    (the doe has tabled a proposal to combine ps  ...\n",
       "4    (this school is an amazing school because we h...\n",
       "Name: tuples, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. [('text', score) . . .]\n",
    "df['tuples'] = list(zip(df.comments, df.compound_binary))\n",
    "df['tuples'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for splitting texts and labels, and loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitions tuples into text and labels for compound_binary values\n",
    "def _prepare_partition(text_label_tuples, *, preprocess=False):\n",
    "    # texts = tuple of sentence strings and labels = tuple of 0 or 1's\n",
    "    # e.g. ('texts are here', 'texts . . . ', . . .)\n",
    "    texts, labels = zip(*text_label_tuples)\n",
    "    # [{'POSITIVE': False, 'NEGATIVE': True}, {'POSITIVE': True, . . .}, {. . .} . . .]\n",
    "    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n",
    "    return texts, cats\n",
    "\n",
    "# limit: how many examples to load from data, dev_size: size of hold-out set\n",
    "def load_data(df, *, limit=0, dev_size=98): \n",
    "    \"\"\"Load data, splitting off a held-out set.\"\"\"\n",
    "    if limit != 0:\n",
    "        limit += dev_size \n",
    "    assert dev_size != 0\n",
    "    \n",
    "    # load training data: df['tuples'] e.g. [('text', score), . . .]\n",
    "    train_data = df    \n",
    "    # len(train_data) = 985 > dev_size = 98\n",
    "    assert len(train_data) > dev_size    \n",
    "    # training data is shuffled\n",
    "    random.shuffle(train_data)\n",
    "    # dev_data = first 98 entries of training data\n",
    "    dev_data = train_data[:dev_size]\n",
    "    # train_data = from 98th entry onwards of training data for length of 887\n",
    "    train_data = train_data[dev_size:]\n",
    "    # partition tuples into text and labels -> train_texts, train_labels\n",
    "    train_texts, train_labels = _prepare_partition(train_data, preprocess=False) \n",
    "    # partition tuples into text and labels -> dev_texts, dev_labels\n",
    "    dev_texts, dev_labels = _prepare_partition(dev_data, preprocess=False)\n",
    "    return (train_texts, train_labels), (dev_texts, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and testing text and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_texts, train_cats), (eval_texts, eval_cats) = load_data(df['tuples'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_trf_bertbaseuncased_lg')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcat = nlp.create_pipe(\"trf_textcat\", config={\"architecture\": \"softmax_class_vector\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add labels to text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # add label to text classifier\n",
    "textcat.add_label(\"POSITIVE\")\n",
    "textcat.add_label(\"NEGATIVE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add classifier to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels:\", textcat.labels)\n",
    "nlp.add_pipe(textcat, last=True)\n",
    "print(f\"Using {len(train_texts)} training docs, {len(eval_texts)} testing docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format train_data as a list of tuples with text at index 0 and a dictionary of labels at index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=4\n",
    "n_texts=1000 \n",
    "batch_size=8 \n",
    "learn_rate=2e-5\n",
    "max_wpb=1000\n",
    "pos_label=\"POSITIVE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for adaptive learning rate <br>\n",
    "Info: https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_triangular_rate(min_lr, max_lr, period):\n",
    "    it = 1\n",
    "    while True:\n",
    "        # https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee\n",
    "        cycle = numpy.floor(1 + it / (2 * period))\n",
    "        x = numpy.abs(it / period - 2 * cycle + 1)\n",
    "        relative = max(0, 1 - x)\n",
    "        yield min_lr + (max_lr - min_lr) * relative\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function for precision, recall, and f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spaCy Language Transformer, texts = eval_texts, cats = eval_cats, pos_label = 'POSITIVE'\n",
    "def evaluate(nlp, texts, cats, pos_label):\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 0.0  # False positives\n",
    "    fn = 0.0  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    total_words = sum(len(text.split()) for text in texts)\n",
    "    with tqdm(total=total_words, leave=False) as pbar:\n",
    "        for i, doc in enumerate(nlp.pipe(texts, batch_size=batch_size)):\n",
    "            gold = cats[i]\n",
    "            for label, score in doc.cats.items():\n",
    "                if label not in gold:\n",
    "                    continue\n",
    "                if label != pos_label:\n",
    "                    continue\n",
    "                if score >= 0.5 and gold[label] >= 0.5:\n",
    "                    tp += 1.0\n",
    "                elif score >= 0.5 and gold[label] < 0.5:\n",
    "                    fp += 1.0\n",
    "                elif score < 0.5 and gold[label] < 0.5:\n",
    "                    tn += 1\n",
    "                elif score < 0.5 and gold[label] >= 0.5:\n",
    "                    fn += 1\n",
    "            pbar.update(len(doc.text.split()))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using stochastic gradient descent data is evaluated and optimized in batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.resume_training()\n",
    "optimizer.alpha = 0.001\n",
    "optimizer.trf_weight_decay = 0.005\n",
    "optimizer.L2 = 0.0\n",
    "learn_rates = cyclic_triangular_rate(\n",
    "    learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size\n",
    "    )\n",
    "print(\"Training the model...\")\n",
    "print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
    "\n",
    "pbar = tqdm(total=100, leave=False)\n",
    "results = []\n",
    "epoch = 0\n",
    "step = 0\n",
    "eval_every = 100\n",
    "patience = 3\n",
    "while True:\n",
    "    # Train and evaluate\n",
    "    losses = Counter()\n",
    "    random.shuffle(train_data)\n",
    "    batches = minibatch(train_data, size=batch_size)\n",
    "    for batch in batches:\n",
    "        optimizer.trf_lr = next(learn_rates)\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, drop=0.1, losses=losses)\n",
    "        pbar.update(1)\n",
    "        if step and (step % eval_every) == 0:\n",
    "            pbar.close()\n",
    "            with nlp.use_params(optimizer.averages):\n",
    "                # nlp = spaCy Language Transformer\n",
    "                scores = evaluate(nlp, eval_texts, eval_cats, pos_label)\n",
    "            results.append((scores[\"textcat_f\"], step, epoch))\n",
    "            print(\n",
    "                \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(\n",
    "                    losses[\"trf_textcat\"],\n",
    "                    scores[\"textcat_p\"],\n",
    "                    scores[\"textcat_r\"],\n",
    "                    scores[\"textcat_f\"],\n",
    "                )\n",
    "            )\n",
    "            pbar = tqdm(total=eval_every, leave=False)\n",
    "        step += 1\n",
    "    epoch += 1\n",
    "    print(f\"epoch {epoch}\")\n",
    "    # Stop if no improvement in HP.patience checkpoints\n",
    "    if results:\n",
    "        best_score, best_step, best_epoch = max(results)\n",
    "        print(f\"best score: {best_score}  best_step : {best_step}  best epoch : {best_epoch} \")\n",
    "        print(f\"break clause: {((step - best_step) // eval_every)}\")\n",
    "        if ((step - best_step) // eval_every) >= patience:\n",
    "            break\n",
    "\n",
    "    msg = wasabi.Printer()\n",
    "    table_widths = [2, 4, 6]\n",
    "    msg.info(f\"Best scoring checkpoints\")\n",
    "    msg.row([\"Epoch\", \"Step\", \"Score\"], widths=table_widths)\n",
    "    msg.row([\"-\" * width for width in table_widths])\n",
    "    for score, step, epoch in sorted(results, reverse=True)[:10]:\n",
    "        msg.row([epoch, step, \"%.2f\" % (score * 100)], widths=table_widths)\n",
    "\n",
    "    # Test the trained model\n",
    "    test_text = eval_texts[0]\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text, doc.cats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
